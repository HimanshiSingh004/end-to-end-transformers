{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps - NLP Lab with Amazon SageMaker\n",
    "\n",
    "**Step 5** - *Automate Inference with the AWS Step Functions Data Science SDK*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "### Install Step Function Data Science SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "import stepfunctions\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow, cloudformation\n",
    "from stepfunctions.steps import Chain, ProcessingStep, TransformStep, Catch, Fail, Succeed\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role() # execution role for SageMaker\n",
    "workflow_execution_role = role # execution role for Step Functions\n",
    "bucket = sagemaker_session.default_bucket() # you can specify a bucket name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_location_fname = '../1_prepare_data/processing_input_location.txt'\n",
    "if os.path.exists(input_location_fname):\n",
    "    with open(input_location_fname, 'r') as f:\n",
    "        processing_input = f.readline()\n",
    "\n",
    "    print(f'Processing input location  | {processing_input}')\n",
    "    \n",
    "else:\n",
    "    print(f'Processing input location file not found ({input_location_fname}): check that the previous notebook was fully executed.')\n",
    "    \n",
    "ecr_image_fname = '../1_prepare_data/ecr_image_name.txt'\n",
    "if os.path.exists(ecr_image_fname):\n",
    "    with open(ecr_image_fname, 'r') as f:\n",
    "        container = f.readline()[:-1]\n",
    "        \n",
    "    print(f'Processing ECR Image ID    | {container}')\n",
    "    \n",
    "else:\n",
    "    print('ECR Image ID not found.')\n",
    "    \n",
    "model_artifact_fname = '../2_train_model/model_artifact_location.txt'\n",
    "if os.path.exists(model_artifact_fname):\n",
    "    with open(model_artifact_fname, 'r') as f:\n",
    "        model_artefact = f.readline()\n",
    "\n",
    "    print(f'Model artifact S3 location | {model_artefact}')\n",
    "    \n",
    "else:\n",
    "    print(f'Model artifact location file not found ({model_artifact_fname}): check that the previous notebook was fully executed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining workflow steps\n",
    "---\n",
    "### Execution input placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = uuid.uuid1().hex\n",
    "processing_output= f's3://{bucket}/{job_name}/data/processed/'\n",
    "transform_input = f's3://{bucket}/{job_name}/data/processed/test_batch_transform.csv'\n",
    "transform_output = f's3://{bucket}/{job_name}/data/predicted/'\n",
    "\n",
    "execution_input = ExecutionInput(\n",
    "    schema={\n",
    "        \"JobName\": str,\n",
    "        \"Processing\": {\n",
    "            \"Input\": str,\n",
    "            \"Output\": str\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = Processor(\n",
    "    role=role, \n",
    "    image_uri=container, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=30, \n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/opt/ml/processing/input'\n",
    "output_folder = '/opt/ml/processing/output'\n",
    "\n",
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='input',\n",
    "        source=execution_input[\"Processing\"][\"Input\"],\n",
    "        destination=input_folder\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name='preprocessed',\n",
    "        source=output_folder,\n",
    "        destination=execution_input[\"Processing\"][\"Output\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker model and data transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(model_data=model_artefact,\n",
    "                     name=name_from_base('bert-model'),\n",
    "                     role=role, \n",
    "                     entry_point='predict_batch.py',\n",
    "                     source_dir='source_dir',\n",
    "                     framework_version='1.5.0')\n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    strategy='SingleRecord',\n",
    "    assemble_with='Line',\n",
    "    accept = 'text/csv',\n",
    "    max_concurrent_transforms=50,\n",
    "    output_path=transform_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling workflow steps\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    state_id=\"Process Data\",\n",
    "    processor=data_processor,\n",
    "    job_name=execution_input[\"JobName\"],\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    container_arguments=[f\"--input={input_folder}\", f\"--output={output_folder}\"],\n",
    "    result_path=\"$.Processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_step = TransformStep(\n",
    "    state_id='Predict Batch',\n",
    "    transformer=transformer,\n",
    "    job_name=execution_input['JobName'],     \n",
    "    model_name=model.name, \n",
    "    data=transform_input,\n",
    "    content_type='text/csv',\n",
    "    split_type='Line',\n",
    "    join_source='Input',\n",
    "    result_path=\"$.Inference\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error catching, failure and success steps:\n",
    "failed = Fail(state_id=\"Failed\")\n",
    "succeed = Succeed(state_id=\"Succeed\")\n",
    "catch_failures = Catch(error_equals=[\"States.ALL\"], next_step=failed)\n",
    "processing_step.add_catch(catch_failures)\n",
    "transformer_step.add_catch(catch_failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create workflow pipeline\n",
    "---\n",
    "By using the Chain utility, we can chain all the above steps together to occur sequentially. We can then choose to output the entire workflow as a JSON, that can be used in a much larger Cloud Formation template for example, which also includes information on the provisioning of instances, setting up of network security etc., or run on its own. By creating the workflow and rendering the graph, a state machine will be created in Amazon Step Functions console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_graph = Chain([\n",
    "        processing_step,\n",
    "        transformer_step,\n",
    "        succeed\n",
    "])\n",
    "\n",
    "workflow_pipeline = Workflow(\n",
    "    name=\"BatchWorkflow\",\n",
    "    definition=workflow_graph,\n",
    "    execution_input=execution_input,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflow_pipeline.definition.to_json(pretty=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_pipeline.render_graph(portrait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/update state machine and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_pipeline.create()\n",
    "# workflow_pipeline.update(workflow_pipeline_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow pipeline inputs\n",
    "---\n",
    "While the following cell is running, cruise over the **[Step Function console](https://eu-west-1.console.aws.amazon.com/states/home)** to check what we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the schema defined at the beginning of this notebook, \n",
    "# we instantiate the Step Function workflow execution input:\n",
    "execution_inputs = {\n",
    "    \"JobName\": job_name, \n",
    "    \"Processing\": {\n",
    "        \"Input\": processing_input, \n",
    "        \"Output\": processing_output\n",
    "    }\n",
    "}\n",
    "\n",
    "workflow_pipeline.execute(inputs=execution_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate CloudFormation template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflow_pipeline.get_cloudformation_template())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
