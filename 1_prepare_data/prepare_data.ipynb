{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps - NLP Lab with Amazon SageMaker\n",
    "**Step 1** - *Prepare data with SageMaker Processing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role() # we are using the notebook instance role for training in this example\n",
    "bucket = sagemaker_session.default_bucket() # you can specify a bucket name here\n",
    "prefix = 'data/input'\n",
    "image_name = 'data-processing-containers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://aws-mlops-workshop.s3-eu-west-1.amazonaws.com/reviews/workshop_data/reviews.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now push this dataset in the default S3 Bucket attached to our SageMaker Notebook instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input = sagemaker_session.upload_data('reviews.csv', bucket, prefix)\n",
    "with open('processing_input_location.txt', 'w') as f:\n",
    "    f.writelines(s3_input)\n",
    "    \n",
    "print(s3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and push Processing container\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already wrote the shell script (located in `docker/build_and_push.sh`) that will build the appropriate Docker container that will be executed by Amazon SageMaker Processing. Processing is a capability of Amazon SageMaker that lets customers easily run the preprocessing, postprocessing and model evaluation workloads on fully managed infrastructure. If you're curious about the actual processing file built into this Docker image, you will find it in `docker/code/prepare_data.py`. Feel free to update this file before building the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./docker/code/prepare_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sh ./docker/build_and_push.sh $image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous script wrote the full ECR Docker Image ID in the ecr_image_name.txt file. Let's read this ID from this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('ecr_image_name.txt'):\n",
    "    with open('ecr_image_name.txt', 'r') as f:\n",
    "        container = f.readline()[:-1]\n",
    "\n",
    "    print(f'ECR Image ID: {container}')\n",
    "else:\n",
    "    print('ECR Image ID not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the data preprocessing job\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we configure a Processor object that will reference the container we just pushed on the Amazon ECR service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = Processor(role=role, \n",
    "                           image_uri=container, \n",
    "                           instance_count=1, \n",
    "                           instance_type='ml.m5.xlarge',\n",
    "                           volume_size_in_gb=30, \n",
    "                           max_runtime_in_seconds=1200,\n",
    "                           base_job_name='data-processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run this processing job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/opt/ml/processing/input'\n",
    "output_folder = '/opt/ml/processing/output'\n",
    "\n",
    "results = data_processor.run(\n",
    "    arguments= [\n",
    "        f'--input={input_folder}',\n",
    "        f'--output={output_folder}'\n",
    "    ],\n",
    "    inputs = [\n",
    "        ProcessingInput(\n",
    "            input_name='input',\n",
    "            source=s3_input,\n",
    "            destination=input_folder\n",
    "        )\n",
    "    ],\n",
    "    outputs= [\n",
    "        ProcessingOutput(\n",
    "            output_name='preprocessed',\n",
    "            source=output_folder,\n",
    "            destination=bucket\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the prepare_data.py script above, this Processing job wrote for files in the output location: we will keep this S3 location handy for the next step where we will train a model using these new CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data_processor.jobs[-1].describe()\n",
    "\n",
    "with open('training_input_location.txt', 'w') as f:\n",
    "    f.writelines(results['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
