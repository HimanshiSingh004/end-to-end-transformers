{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate Training with the AWS Step Functions Data Science SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch as PyTorchEstimator\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "import stepfunctions\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow, cloudformation\n",
    "from stepfunctions.steps import Chain, ProcessingStep, TrainingStep, Catch, Fail, Succeed\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role() # execution role for SageMaker\n",
    "workflow_execution_role = role # execution role for Step Functions\n",
    "bucket = sagemaker_session.default_bucket() # you can specify a bucket name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution input placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = uuid.uuid1().hex\n",
    "processing_input = '<your-data-s3-path>' \n",
    "processing_output= f's3://{bucket}/{job_name}/data/processed/'\n",
    "\n",
    "\n",
    "execution_input = ExecutionInput(\n",
    "    schema={\n",
    "        \"JobName\": str,\n",
    "        \"Processing\": {\n",
    "            \"Input\": str,\n",
    "            \"Output\": str\n",
    "        },\n",
    "        \"Training\": {\n",
    "            \"Input\": str,\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = Processor(\n",
    "    role=role, \n",
    "    image_uri='<your-container-image-uri>', \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=30, \n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/opt/ml/processing/input'\n",
    "output_folder = '/opt/ml/processing/output'\n",
    "\n",
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name='input',\n",
    "        source=execution_input[\"Processing\"][\"Input\"],\n",
    "        destination=input_folder\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name='preprocessed',\n",
    "        source=output_folder,\n",
    "        destination=execution_input[\"Processing\"][\"Output\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model_name\":\"bert-base-cased\",\n",
    "    \"data_folder\": '/opt/ml/input/data/train',\n",
    "    \"output_folder\": '/opt/ml/model',\n",
    "    \"epochs\": 1,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 64,\n",
    "    \"seed\": 42,\n",
    "    \"max_len\": 160\n",
    "}\n",
    "\n",
    "metric_definitions = [{'Name': 'validation_accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'}]\n",
    "\n",
    "estimator = PyTorchEstimator(\n",
    "    entry_point='train.py',\n",
    "    source_dir='source_dir',\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    train_volume_size=50,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    framework_version='1.5.0',\n",
    "    py_version='py3',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define workflow steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    state_id=\"Process Data\",\n",
    "    processor=data_processor,\n",
    "    job_name=execution_input[\"JobName\"],\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    container_arguments=[f\"--input={input_folder}\", f\"--output={output_folder}\"],\n",
    "    result_path=\"$.Processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = TrainingStep(\n",
    "    state_id =\"Train Model\",\n",
    "    estimator=estimator,\n",
    "    job_name=execution_input[\"JobName\"],\n",
    "    data={'train': processing_output},\n",
    "    result_path=\"$.Training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error catching, failure and success steps\n",
    "failed = Fail(state_id=\"Failed\")\n",
    "succeed = Succeed(state_id=\"Succeed\")\n",
    "\n",
    "catch_failures = Catch(error_equals=[\"States.ALL\"], next_step=failed)\n",
    "processing_step.add_catch(catch_failures)\n",
    "training_step.add_catch(catch_failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create workflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_graph = Chain([\n",
    "        processing_step,\n",
    "        training_step,\n",
    "        succeed\n",
    "])\n",
    "\n",
    "workflow_pipeline = Workflow(\n",
    "    name=\"TrainingWorkflow\",\n",
    "    definition=workflow_graph,\n",
    "    execution_input=execution_input,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflow_pipeline.definition.to_json(pretty=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_pipeline.render_graph(portrait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create/update state machine and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_pipeline.create()\n",
    "# workflow_pipeline.update(workflow_pipeline_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow pipeline inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_inputs = {\n",
    "    \"JobName\": job_name, \n",
    "    \"Processing\": {\n",
    "        \"Input\": processing_input, \n",
    "        \"Output\": processing_output\n",
    "    }, \n",
    "    \"Training\": {\n",
    "        \"Input\": processing_output\n",
    "    }\n",
    "}\n",
    "\n",
    "workflow_pipeline.execute(inputs=execution_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CloudFormation template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflow_pipeline.get_cloudformation_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
